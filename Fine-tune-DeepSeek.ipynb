{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fba47-28c4-4d8d-b684-c5ca632b6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233a177-2be5-4465-aab6-16ca66c135e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"YY/DeepSeek-R1-reviewer-Csharp\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_16bit=True,\n",
    "    token=\"\"\n",
    ")\n",
    "\n",
    "CHECKPOINT_PATH = \"/home/vm-admin/CodeReviewer-Model/outputs/checkpoint-3822\"\n",
    "model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb59f6d-db70-4b9a-a356-23335bdb03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_token = \"\"\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B lastRun-ever', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241ff26-97cb-4e42-bee5-c64e8046abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    model_name = \"B/DeepSeek-R1-reviewer-Csharp\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f59bc-e481-4330-a6bc-6f61af21e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastLanguageModel.for_inference(model)\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task,\n",
    "paired with an input that provides further context. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\n",
    "\"\"\"\n",
    "\n",
    "instruction_text = \"You are a powerful code reviewer model for the c# programming language. Your job is to suggest 1 review comment in natural language. You are given a context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\"\n",
    "diff_hunk = \"\"\"Diff Hunk: \"@@ -985,8 +984,9 @@ namespace Lovion.PlugIns.XXX.Web\\n                                                                 totalSumItem.TotalRunSetupDays,\\n                                                                 totalSumItem.TotalWorkdays,\\n                                                                 // if the number of tasks exceeds the defined LIMIT, don't retrieve them as they can't be displayed\\n-                                                                totalSumItem.Tasks.Count() <= TASK_DETAILS_OBJECTS_COUNT_LIMIT ? totalSumItem.Tasks.Select(WebConverter.ToWebRwoId).ToList() : new List<WebRwoId>(),\\n-                                                                totalSumItem.Tasks.Count(),\\n+                                                                (totalSumItem.Tasks != null && totalSumItem.NumberOfTasks <= TASK_DETAILS_OBJECTS_COUNT_LIMIT)\\n+                                                                    ? totalSumItem.Tasks.Select(WebConverter.ToWebRwoId).ToList() : new List<WebRwoId>(),\\n+                                                                totalSumItem.NumberOfTasks,\\n                                                                 Components.TaskWork.Properties.CommonResources.Total);\\n                 }\\n \\n\"\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt = prompt_style.format(\n",
    "    instruction=instruction_text,\n",
    "    input=diff_hunk,\n",
    "    output=\"\" \n",
    ")\n",
    "\n",
    "inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "response_part = decoded_text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Raw Generation:\\n\", decoded_text)\n",
    "#print(\"\\nFinal Extracted Response:\\n\", response_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22c693-03f7-4f5c-b1df-e2638e4c1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import sacrebleu\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_16bit = True\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"\" \n",
    ")\n",
    "model.eval()\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt_style = \"\"\"Below is an instruction that describes a task,\n",
    "paired with an input that provides further context. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "test_data = []\n",
    "with open(\"/home/vm-admin/CodeReviewer-Model/CodeLlama/dataset/test_alpaca.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for example in test_data:\n",
    "    instruction_text = example[\"instruction\"]\n",
    "    diff_hunk = example[\"input\"]\n",
    "    reference_comment = example[\"output\"]\n",
    "\n",
    "    inference_prompt = prompt_style.format(\n",
    "        instruction=instruction_text,\n",
    "        input=diff_hunk,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Try: remove the prompt, “### Response:”, etc.\n",
    "    # if the model repeats them\n",
    "    # if \"### Response:\" in generated_text:\n",
    "    #     generated_text = generated_text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    references.append([reference_comment])\n",
    "    predictions.append(generated_text)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
    "print(f\"BLEU score: {bleu.score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b784d-1753-4bda-a28a-7925efe3d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task,\n",
    "paired with an input that provides further context. Write a response\n",
    "that appropriately completes the request. \n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for inst, inp, out in zip(instructions, inputs, outputs):\n",
    "        text = train_prompt_style.format(inst, inp, out) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "    \n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"/home/vm-admin/CodeReviewer-Model/CodeLlama/dataset/train-alpaca.jsonl\",\n",
    "        \"valid\": \"/home/vm-admin/CodeReviewer-Model/CodeLlama/dataset/valid_alpaca.jsonl\",\n",
    "        \"test\":  \"/home/vm-admin/CodeReviewer-Model/CodeLlama/dataset/test_alpaca.jsonl\",\n",
    "    }\n",
    ")\n",
    "print(type(dataset[\"train\"]))\n",
    "print(dataset[\"train\"][0])\n",
    "dataset[\"train\"] = dataset[\"train\"].map(formatting_prompts_func, batched=True)\n",
    "dataset[\"valid\"] = dataset[\"valid\"].map(formatting_prompts_func, batched=True)\n",
    "dataset[\"test\"]  = dataset[\"test\"].map(formatting_prompts_func, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8d446-2b4d-4e23-85db-f0b273302e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=2233,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5801b40-1ae0-4bde-8198-f6440e9ef55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=16,  #24 cpu cores are in the vm available\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=10,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1500,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=2233,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01b472-5515-4b65-a5b1-692ee7519cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=\"outputs/checkpoint-3822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06ebbb-d5d4-4b19-8c3d-c228e10489ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    "    token=\"\"\n",
    ")\n",
    "\n",
    "CHECKPOINT_PATH = \"/home/vm-admin/CodeReviewer-Model/outputs/checkpoint-5733\"\n",
    "model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33b5d5-8506-4bad-ab75-97084aa7746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task,\n",
    "paired with an input that provides further context. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\n",
    "\"\"\"\n",
    "\n",
    "instruction_text = \"You are a powerful code reviewer model for the c# programming language. Your job is to suggest 1 review comment in natural language. You are given a context regarding a diff hunk or code change in programming language. You must output appropriate, contextual review comment for that code change.\"\n",
    "diff_hunk = \"\"\"Diff Hunk:@@ -148,10 +143,9 @@ namespace XX.GuiComponents.RwoGrid\\n             var newRwoTypes = Enumerable.Empty<IRwoType>();\\r\\n             if (rwos != null)\\r\\n             {\\r\\n-                newRwoTypes = rwos.Select(r => r.RwoType)\\r\\n-                                  .Distinct();\\r\\n+                newRwoTypes = rwos.Select(r => r.RwoType);\\r\\n \\r\\n-                dataGrid._CurrentRwoTypes = new HashSet<IRwoType>(newRwoTypes);\\r\\n+                dataGrid._CurrentRwoTypes = newRwoTypes.ToHashSet();\\r\\n             }\\r\\n             return newRwoTypes;\\r\\n         }\\r\\n\",\"\"\"\n",
    "\n",
    "inference_prompt = prompt_style.format(\n",
    "    instruction=instruction_text,\n",
    "    input=diff_hunk,\n",
    "    output=\"\" \n",
    ")\n",
    "\n",
    "inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "response_part = decoded_text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Raw Generation:\\n\", decoded_text)\n",
    "print(\"\\nFinal Extracted Response:\\n\", response_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f1dd0-c9a4-4666-b6b3-0f4daf8a1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_online = \"B/DeepSeek-R1-reviewer-Csharp\"\n",
    "model.push_to_hub(new_model_online)\n",
    "tokenizer.push_to_hub(new_model_online)\n",
    "\n",
    "model.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_4bit_forced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994035f-dbd1-4fe4-b7a0-0f49ee8ced0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_online = \"B/DeepSeek-R1-reviewer-Csharp-16bit\"\n",
    "model.push_to_hub(new_model_online)\n",
    "tokenizer.push_to_hub(new_model_online)\n",
    "\n",
    "model.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
