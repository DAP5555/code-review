{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd8739-0c4f-4704-946d-a0661b9a6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar script to what was used for data-fetching\n",
    "#!/usr/bin/env python3\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import io\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "from unidiff import PatchSet\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "GITEA_URL = \"\"\n",
    "OWNER = \"\"\n",
    "REPO = \"\"\n",
    "\n",
    "# Replace with your actual API token,\n",
    "API_TOKEN = \"\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"token {API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "base_api_url = f\"{GITEA_URL}/api/v1\"\n",
    "\n",
    "\n",
    "def get_all_pull_requests(state=\"all\", limit=50):\n",
    "    \"\"\"\n",
    "    Fetch *all* pull requests for the repo, using pagination.\n",
    "    \"\"\"\n",
    "    pulls_url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/pulls\"\n",
    "    pulls_url += f\"?state={state}&limit={limit}\"\n",
    "\n",
    "    pull_requests = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        paged_url = f\"{pulls_url}&page={page}\"\n",
    "        try:\n",
    "            resp = requests.get(paged_url, headers=headers, timeout=120)\n",
    "            resp.raise_for_status()\n",
    "            current_page_pulls = resp.json()\n",
    "            if not current_page_pulls:\n",
    "                break\n",
    "            pull_requests.extend(current_page_pulls)\n",
    "            logging.info(f\"Fetched page {page} with {len(current_page_pulls)} pull requests.\")\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching pull requests on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    return pull_requests\n",
    "\n",
    "\n",
    "def get_pr_data(pr_number):\n",
    "    url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/pulls/{pr_number}\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"[PR #{pr_number}] Error fetching PR data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_pr_diff(pr_number):\n",
    "    url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/pulls/{pr_number}.diff\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\n",
    "            \"Authorization\": f\"token {API_TOKEN}\",\n",
    "            \"Accept\": \"text/plain\"\n",
    "        }, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        return resp.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"[PR #{pr_number}] Error fetching diff: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_reviews(pr_number):\n",
    "    url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/pulls/{pr_number}/reviews\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"[PR #{pr_number}] Error fetching reviews: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_review_comments(pr_number, review_id):\n",
    "    url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/pulls/{pr_number}/reviews/{review_id}/comments\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"[PR #{pr_number}] Error fetching review #{review_id} comments: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_file_content_at_commit(file_path, commit_sha):\n",
    "    encoded_path = quote(file_path, safe=\"\")\n",
    "    url = f\"{base_api_url}/repos/{quote(OWNER)}/{quote(REPO)}/contents/{encoded_path}?ref={commit_sha}\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=120)\n",
    "        if resp.status_code == 200:\n",
    "            content_json = resp.json()\n",
    "            encoded_content = content_json.get('content')\n",
    "            if encoded_content:\n",
    "                return base64.b64decode(encoded_content).decode('utf-8', errors='replace')\n",
    "        else:\n",
    "            logging.warning(f\"[Commit {commit_sha}] File not found: {file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"[Commit {commit_sha}] Error fetching file {file_path}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def handle_pr(pr):\n",
    "    \"\"\"\n",
    "    Process a single PR:\n",
    "      - Get PR data for base commit\n",
    "      - Fetch the .diff and parse it\n",
    "      - Collect all review comments by (file_path, line_number)\n",
    "      - Iterate over patches/hunks, output only those with comments\n",
    "      - Return a list of JSON-serializable objects\n",
    "    \"\"\"\n",
    "    pr_number = pr[\"number\"]\n",
    "    pr_data = get_pr_data(pr_number)\n",
    "    if not pr_data:\n",
    "        return []\n",
    "\n",
    "    base_sha = pr_data[\"base\"][\"sha\"]\n",
    "    diff_text = get_pr_diff(pr_number)\n",
    "    if not diff_text:\n",
    "        return []\n",
    "\n",
    "    patch = PatchSet(io.StringIO(diff_text))\n",
    "\n",
    "    # Collect review comments\n",
    "    reviews = get_reviews(pr_number)\n",
    "    comments_by_file_and_line = {}\n",
    "\n",
    "    for review in reviews:\n",
    "        review_id = review['id']\n",
    "        review_comments = get_review_comments(pr_number, review_id)\n",
    "        for c in review_comments:\n",
    "            file_path = c.get('path')\n",
    "            diff_line_number = c.get('line') or c.get('position')\n",
    "            body = c.get('body', \"\")\n",
    "            if file_path and diff_line_number is not None:\n",
    "                comments_by_file_and_line.setdefault((file_path, diff_line_number), []).append(body)\n",
    "\n",
    "    # Now iterate over each file/hunk to find commented lines\n",
    "    results = []\n",
    "    for patched_file in patch:\n",
    "        old_file_path = patched_file.source_file[2:] if patched_file.source_file.startswith(\"a/\") else patched_file.source_file\n",
    "        new_file_path = patched_file.target_file[2:] if patched_file.target_file.startswith(\"b/\") else patched_file.target_file\n",
    "\n",
    "        old_file_content = get_file_content_at_commit(old_file_path, base_sha)\n",
    "        if old_file_content is None:\n",
    "            old_file_content = \"\"\n",
    "\n",
    "        for hunk in patched_file:\n",
    "            patch_str = str(hunk)  # includes the @@ ... @@ header and +/- lines\n",
    "\n",
    "            all_comments = []\n",
    "            for line in hunk:\n",
    "                t_line_no = line.target_line_no\n",
    "                if (new_file_path, t_line_no) in comments_by_file_and_line:\n",
    "                    all_comments.extend(comments_by_file_and_line[(new_file_path, t_line_no)])\n",
    "\n",
    "            if not all_comments:\n",
    "                continue\n",
    "\n",
    "            combined_msg = \"\\n\".join(all_comments)\n",
    "            data_entry = {\n",
    "                \"oldf\": old_file_content,\n",
    "                \"patch\": patch_str,\n",
    "                \"msg\": combined_msg,\n",
    "                \"id\": pr_number,\n",
    "                \"y\": 1\n",
    "            }\n",
    "            results.append(data_entry)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Fetching all pull requests (state=all, limit=50) ...\")\n",
    "    pull_requests = get_all_pull_requests(state=\"all\", limit=50)\n",
    "    logging.info(f\"Total PRs fetched: {len(pull_requests)}\")\n",
    "\n",
    "    output_filename = \".jsonl\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as outf:\n",
    "        for pr in pull_requests:\n",
    "            pr_num = pr[\"number\"]\n",
    "            logging.info(f\"Processing PR #{pr_num} ...\")\n",
    "            hunk_data_list = handle_pr(pr)\n",
    "            for data_entry in hunk_data_list:\n",
    "                json.dump(data_entry, outf, ensure_ascii=False)\n",
    "                outf.write(\"\\n\")\n",
    "\n",
    "    logging.info(f\"Done. Output written to {output_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a6a40-1c8b-42ee-8f2c-4e7e46939f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate natural language\n",
    "import json\n",
    "from googletrans import Translator\n",
    "\n",
    "def extract_and_translate_jsonl(\n",
    "    input_jsonl_path= r\"directory\",\n",
    "    translated_txt_path= r\"directory\"\n",
    "):\n",
    "    translator = Translator()\n",
    "\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(translated_txt_path, \"w\", encoding=\"utf-8\") as txtfile:\n",
    "        \n",
    "        line_count = 0\n",
    "        \n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                txtfile.write(\"\\n\")\n",
    "                continue\n",
    "            \n",
    "            line_count += 1\n",
    "            \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                txtfile.write(f\"<JSON_DECODE_ERROR: {str(e)}>\\n\")\n",
    "                continue\n",
    "\n",
    "            msg = record.get(\"msg\", \"\")\n",
    "            \n",
    "            if not msg:\n",
    "                txtfile.write(\"\\n\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                translation = translator.translate(msg, dest='en')\n",
    "                if not translation or translation.text is None:\n",
    "                    english_msg = msg\n",
    "                else:\n",
    "                    english_msg = translation.text\n",
    "            except Exception as e:\n",
    "                english_msg = f\"<TRANSLATION_ERROR: {str(e)}> Original: {msg}\"\n",
    "            \n",
    "            english_msg = english_msg.replace(\"\\n\", \" \")\n",
    "            \n",
    "            txtfile.write(english_msg + \"\\n\")\n",
    "        \n",
    "        print(f\"Translation complete! Processed {line_count} JSON lines.\")\n",
    "        print(\"Check and edit the output in\", translated_txt_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_translate_jsonl(\n",
    "        input_jsonl_path=r\"\",\n",
    "        translated_txt_path=r\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32008e03-b8f3-49ec-948a-9c15e85432d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script that compares if training data and test data might have the same objects\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_identifier(obj, fields):\n",
    "    try:\n",
    "        return tuple(obj[field] for field in fields)\n",
    "    except KeyError as e:\n",
    "        missing_field = e.args[0]\n",
    "        return None\n",
    "\n",
    "def load_identifiers(file_path, fields):\n",
    "    identifiers = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  \n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                identifier = build_identifier(obj, fields)\n",
    "                if identifier:\n",
    "                    identifiers.add(identifier)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {line_number} in {file_path}: {e}\")\n",
    "    return identifiers\n",
    "\n",
    "def find_duplicates(training_file, testing_file, output_file, fields):\n",
    "    print(\"Loading training data identifiers...\")\n",
    "    training_identifiers = load_identifiers(training_file, fields)\n",
    "    print(f\"Total unique identifiers in training data: {len(training_identifiers)}\")\n",
    "    \n",
    "    duplicates_count = 0\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    print(\"Comparing testing data against training data identifiers...\")\n",
    "    with open(testing_file, 'r', encoding='utf-8') as test_file, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as dup_file:\n",
    "        \n",
    "        for line_number, line in enumerate(test_file, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue \n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                identifier = build_identifier(obj, fields)\n",
    "                if identifier and identifier in training_identifiers:\n",
    "                    dup_file.write(line + '\\n')\n",
    "                    duplicates_count += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {line_number} in {testing_file}: {e}\")\n",
    "    \n",
    "    return duplicates_count\n",
    "\n",
    "def main():\n",
    "    training_file_path = r\"\"\n",
    "    testing_file_path = r\"\"\n",
    "    duplicates_output_path = r\"\"\n",
    "    \n",
    "    compare_fields = [\"oldf\", \"patch\", \"msg\", \"id\"]\n",
    "    \n",
    "    duplicates_found = find_duplicates(\n",
    "        training_file=training_file_path,\n",
    "        testing_file=testing_file_path,\n",
    "        output_file=duplicates_output_path,\n",
    "        fields=compare_fields\n",
    "    )\n",
    "    \n",
    "    print(f\"Duplicate comparison completed.\")\n",
    "    print(f\"Total duplicates found: {duplicates_found}\")\n",
    "    print(f\"Duplicates have been written to: {duplicates_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cfa98-e078-4988-88f0-95e1ded78333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to prepare the alpaca data format\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = r''\n",
    "output_file_path = r''\n",
    "\n",
    "instruction_text = (\n",
    "    \"You are a powerful code reviewer model for the c# programming language. Your job is to suggest \"\n",
    "    \"review comments in natural language. You are given a context \"\n",
    "    \"regarding a diff hunk (code change) in programming language. \"\n",
    "    \"You must output appropriate, contextual review comment for that code change.\"\n",
    ")\n",
    "\n",
    "# Process and convert the JSONL file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    for line in tqdm(infile, desc=\"Converting JSONL to Alpaca Format\"):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping invalid JSON line: {e}\")\n",
    "            continue\n",
    "        patch = data.get('patch', '').strip()\n",
    "        msg = data.get('msg', '').strip()\n",
    "        \n",
    "        new_obj = {\n",
    "            \"instruction\": instruction_text,\n",
    "            \"input\": f\"Diff Hunk:\\n{patch}\",\n",
    "            \"output\": msg\n",
    "        }\n",
    "        \n",
    "        json_output = json.dumps(new_obj, ensure_ascii=False)\n",
    "        outfile.write(json_output + '\\n')\n",
    "\n",
    "print(f\"Conversion completed. Output saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d204563-6781-432b-8287-e8afe0bfebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two jsonl files into one and shuffle them\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def merge_and_shuffle_jsonl(\n",
    "    input_file1,\n",
    "    input_file2,\n",
    "    output_file,\n",
    "    seed=None\n",
    "):\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        print(f\"Random seed set to {seed}.\")\n",
    "\n",
    "    for file_path in [input_file1, input_file2]:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"Input file '{file_path}' does not exist.\")\n",
    "\n",
    "    print(f\"Reading lines from '{input_file1}'...\")\n",
    "    with open(input_file1, 'r', encoding='utf-8') as file1:\n",
    "        lines1 = file1.readlines()\n",
    "    print(f\"Number of lines in '{input_file1}': {len(lines1)}\")\n",
    "\n",
    "    print(f\"Reading lines from '{input_file2}'...\")\n",
    "    with open(input_file2, 'r', encoding='utf-8') as file2:\n",
    "        lines2 = file2.readlines()\n",
    "    print(f\"Number of lines in '{input_file2}': {len(lines2)}\")\n",
    "\n",
    "    combined_lines = lines1 + lines2\n",
    "    print(f\"Total lines after merging: {len(combined_lines)}\")\n",
    "\n",
    "    print(\"Shuffling the combined lines...\")\n",
    "    random.shuffle(combined_lines)\n",
    "\n",
    "    print(f\"Writing shuffled lines to '{output_file}'...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(combined_lines)\n",
    "\n",
    "    print(\"Merging and shuffling completed successfully!\")\n",
    "\n",
    "def validate_jsonl(file_path, sample_size=5):\n",
    "    import json\n",
    "\n",
    "    print(f\"Validating JSON objects in '{file_path}'...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Invalid JSON on line {i+1}: {e}\")\n",
    "                return False\n",
    "    print(f\"First {sample_size} lines are valid JSON objects.\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILE1 = r\"\" \n",
    "    INPUT_FILE2 = r\"\"\n",
    "    OUTPUT_FILE = r\"\"\n",
    "\n",
    "    SEED = 42  # example\n",
    "\n",
    "    merge_and_shuffle_jsonl(\n",
    "        input_file1=INPUT_FILE1,\n",
    "        input_file2=INPUT_FILE2,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    is_valid = validate_jsonl(OUTPUT_FILE, sample_size=10)\n",
    "    if is_valid:\n",
    "        print(f\"Validation passed for '{OUTPUT_FILE}'.\")\n",
    "    else:\n",
    "        print(f\"Validation failed for '{OUTPUT_FILE}'. Please check the file for errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1fc4-775e-4a39-a5ca-a005ed5a38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperates a script into 4 parts for the cls task in the codeReviwer\n",
    "import random\n",
    "import os\n",
    "\n",
    "def split_random_lines(\n",
    "    input_file_path,\n",
    "    output_file_paths,\n",
    "    total_selected_lines,\n",
    "    split_counts,\n",
    "    seed=None\n",
    "):\n",
    "    if len(output_file_paths) != len(split_counts):\n",
    "        raise ValueError(\"The number of output files must match the number of split counts.\")\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed) \n",
    "    if not os.path.isfile(input_file_path):\n",
    "        raise FileNotFoundError(f\"Input file '{input_file_path}' does not exist.\")\n",
    "\n",
    "    print(f\"Reading lines from '{input_file_path}'...\")\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "        all_lines = infile.readlines()\n",
    "\n",
    "    total_available = len(all_lines)\n",
    "    print(f\"Total lines available in input file: {total_available}\")\n",
    "\n",
    "    if total_selected_lines > total_available:\n",
    "        raise ValueError(\n",
    "            f\"Requested {total_selected_lines} lines, but only {total_available} are available.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Selecting {total_selected_lines} random lines...\")\n",
    "    selected_lines = random.sample(all_lines, total_selected_lines)\n",
    "\n",
    "    print(\"Splitting selected lines into output files...\")\n",
    "    current_index = 0\n",
    "    for output_path, count in zip(output_file_paths, split_counts):\n",
    "        subset = selected_lines[current_index:current_index + count]\n",
    "        print(f\"Writing {count} lines to '{output_path}'...\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.writelines(subset)\n",
    "        current_index += count\n",
    "\n",
    "    print(\"Splitting completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_FILE = r\"\"\n",
    "\n",
    "    OUTPUT_FILES = [ # Set 4 dir files\n",
    "    ]\n",
    "\n",
    "    TOTAL_SELECTED_LINES = 19111\n",
    "\n",
    "    SPLIT_COUNTS = [4778, 4778, 4777, 4778] # example\n",
    "\n",
    "    SEED = 42  # example\n",
    "\n",
    "    split_random_lines(\n",
    "        input_file_path=INPUT_FILE,\n",
    "        output_file_paths=OUTPUT_FILES,\n",
    "        total_selected_lines=TOTAL_SELECTED_LINES,\n",
    "        split_counts=SPLIT_COUNTS,\n",
    "        seed=SEED\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1bc22-f5dd-4292-bd4a-790171316195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script checks the number of json objects that have y:1 or y: 0\n",
    "import json\n",
    "\n",
    "def count_y_zero(file_path):\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                if obj.get('y') == 1:\n",
    "                    count += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {line_number}: {e}\")\n",
    "    return count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r\"\"   \n",
    "    total = count_y_zero(file_path)\n",
    "    print(f\"Total objects with \\\"y\\\": 1 -> {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ade83-5d4a-4136-bb9c-04a774bd15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the jsonl into alpaca format for codeLlama\n",
    "import json\n",
    "\n",
    "new_data = []\n",
    "with open(\"original_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        old_obj = json.loads(line)\n",
    "        \n",
    "        new_obj = {\n",
    "            \"instruction\": (\n",
    "                \"You are a powerful c# code reviewer model. Your job is to suggest \"\n",
    "                \"a review comment in natural language. You are given a context \"\n",
    "                \"regarding a diff hunk or code change in programming language. \"\n",
    "                \"You must output appropriate, contextual review comment for that code change.\"\n",
    "            ),\n",
    "            \"input\": f\"Diff Hunk:\\n{old_obj['patch']}\",\n",
    "            \"output\": old_obj[\"msg\"]\n",
    "        }\n",
    "        \n",
    "        new_data.append(new_obj)\n",
    "\n",
    "with open(\"converted_alpaca_format.jsonl\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for item in new_data:\n",
    "        fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd3891-248c-4f2b-b383-92c3160023e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "input_file = r''\n",
    "\n",
    "with open(input_file, 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(f\"Detected Encoding: {result['encoding']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85502e6a-3ca5-4d1e-9772-b0b7749168d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean json- objects from BOMs\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "def clean_string(s):\n",
    "    # Remove BOM if present\n",
    "    if s.startswith('\\ufeff'):\n",
    "        print(\"Found BOM at the start of the string. Removing it.\")\n",
    "        s = s.lstrip('\\ufeff')\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def remove_boms_and_validate_utf8(input_file, output_file, field_to_process):\n",
    "    try:\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"Error: The input file '{input_file}' does not exist.\", file=sys.stderr)\n",
    "            return\n",
    "        \n",
    "        with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "             open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            \n",
    "            line_num = 0\n",
    "            issues_found = 0\n",
    "            for line in infile:\n",
    "                line_num += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    json_obj = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Warning: Skipping invalid JSON on line {line_num}: {e}\", file=sys.stderr)\n",
    "                    issues_found += 1\n",
    "                    continue\n",
    "\n",
    "                if field_to_process in json_obj:\n",
    "                    original_value = json_obj[field_to_process]\n",
    "                    if not isinstance(original_value, str):\n",
    "                        print(f\"Warning: The field '{field_to_process}' on line {line_num} is not a string.\", file=sys.stderr)\n",
    "                        issues_found += 1\n",
    "                    else:\n",
    "                        cleaned_value = clean_string(original_value)\n",
    "                        if cleaned_value != original_value:\n",
    "                            print(f\"Info: Cleaned the field '{field_to_process}' on line {line_num}.\", file=sys.stderr)\n",
    "                            issues_found += 1\n",
    "                        json_obj[field_to_process] = cleaned_value\n",
    "\n",
    "                try:\n",
    "                    json_line = json.dumps(json_obj, ensure_ascii=False)\n",
    "                    outfile.write(json_line + '\\n')\n",
    "                except (TypeError, OverflowError) as e:\n",
    "                    print(f\"Warning: Could not serialize JSON object on line {line_num}: {e}\", file=sys.stderr)\n",
    "                    issues_found += 1\n",
    "                    continue\n",
    "\n",
    "            print(f\"Processing complete. Modified JSONL written to '{output_file}'.\")\n",
    "            if issues_found > 0:\n",
    "                print(f\"Note: {issues_found} issues were found during processing. Check the warnings above for details.\", file=sys.stderr)\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "    except IOError as e:\n",
    "        print(f\"I/O error: {e}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "input_file_path =\n",
    "output_file_path =\n",
    "\n",
    "field_to_process = 'oldf' # field that we need to clean\n",
    "\n",
    "remove_boms_and_validate_utf8(input_file_path, output_file_path, field_to_process)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
